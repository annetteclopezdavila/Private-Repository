# -*- coding: utf-8 -*-
"""FINAL Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lYQ8y-HGv2OndMHPZgJHdTS57kjIzt7g

# Import & Preprocess DF
"""

from google.colab import drive
drive.mount('/content/gdrive')

import tensorflow as tf
import numpy as np
import glob
import re
from PIL import Image

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd #load pandas library
#read from file name .csv
#file = np.genfromtxt('/content/gdrive/My Drive/Final.csv', delimiter=',')
#file
filename='/content/gdrive/My Drive/Final.csv'
df=pd.read_csv(filename, header=None, sep=',')
header_row=0
df.columns=df.iloc[header_row]
df=df.drop(header_row)
df.head()

df=df.dropna(axis=1, how='all')
df

X=df.drop('Disorder', axis=1)
y=df['Disorder']

X['Systolic Blood Pressure']=pd.to_numeric(X['Systolic Blood Pressure'])
X['Ionized Magnesium Levels mmol/L']=pd.to_numeric(X['Ionized Magnesium Levels mmol/L'])
X['Amygdala Relative Volume (%)']=pd.to_numeric(X['Amygdala Relative Volume (%)'])
X['neocortical 5-HT4 receptor binding']=pd.to_numeric(X['neocortical 5-HT4 receptor binding'])
X['Cortisol Levels mcg/dL']=pd.to_numeric(X['Cortisol Levels mcg/dL'])

X['LDL Cholesterol mg/dL']=pd.to_numeric(X['LDL Cholesterol mg/dL'])
X['ceramides ng/ml']=pd.to_numeric(X['ceramides ng/ml'])
X['hippocampus mm^3']=pd.to_numeric(X['hippocampus mm^3'])
X['Glutamate ?mol/L']=pd.to_numeric(X['Glutamate ?mol/L'])

"""# Correlation Matrix"""

corr_matrix=np.corrcoef(X)
corr_matrix

"""# Heatmap"""

import seaborn as sns
plt.figure(figsize=(12,12))
sns.heatmap(X.corr(), vmax=1, vmin=-1, cmap="spring", annot=True,fmt='.2f')
plt.show()

"""# Pairplot"""

#sns.pairplot(data=df, kind='scatter', hue='Disorder')
#plt.show()

"""# Scaled PCA"""

from sklearn.preprocessing import StandardScaler
#initialize a neew standard scalar
ss=StandardScaler()
X_scaled=ss.fit_transform(X)
X_names=['Systolic Blood Pressure', 'Ionized Magnesium Levels mmol/L','Amygdala Relative Volume (%)','neocortical 5-HT4 receptor binding','Cortisol Levels mcg/dL','LDL Cholesterol mg/dL','ceramides ng/ml', 'hippocampus mm^3', 'Glutamate ?mol/L']

X_scaled_df=pd.DataFrame(X_scaled, columns=X_names)
X_scaled_df.head()

#SCALED PCA
from sklearn.decomposition import PCA
#initializing a new PCA object
pca= PCA()
X_pca=pca.fit_transform(X_scaled_df)
y_names=['Migraine','Anxiety']
pca.explained_variance_ratio_

PCADataFrame=pd.DataFrame(data=X_pca,columns=['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9'])
#plt.figure(figsize=(8,8)
PCADataFrame.head()

target_df = pd.DataFrame(y, columns= ['Disorder'])

PCADataFrame = pd.concat([PCADataFrame, target_df], axis= 1)
PCADataFrame.head()

PCADataFrame.fillna('Migraine')

PCADataFrame.columns
sns.scatterplot(PCADataFrame['PC1'], PCADataFrame['PC2'], hue='Disorder', data=PCADataFrame, palette="hot")

evr=pca.explained_variance_ratio_
plt.figure(figsize=(12,6))
plt.bar(range(1, len(evr)+1), evr)
plt.xlabel('Principal component')
plt.ylabel('% Variance Explained')
plt.show()

#not too great only 45% explained by first two

"""# SCALED TSNE"""

from sklearn.manifold import TSNE
tsne=TSNE (random_state=146)
# Perform the transformation (we'll use the default settings, and do it on the standardized data)
X_scaled_tsne=tsne.fit_transform(X_scaled_df)

TSNEDataFrame=pd.DataFrame(data=X_scaled_tsne,columns=['TSNE1','TSNE2'])
TSNEDataFrame

TSNEDataFrame = pd.concat([TSNEDataFrame, target_df], axis= 1)
TSNEDataFrame.fillna('Migraine').head()

TSNEDataFrame.head()

sns.scatterplot(TSNEDataFrame['TSNE1'], TSNEDataFrame['TSNE2'], hue='Disorder', data=PCADataFrame, palette="hot")

"""# Definitions"""

def GetColors(N, map_name='rainbow'):
    '''Function returns a list of N colors from a matplotlib colormap
            Input: N = number of colors, and map_name = name of a matplotlib colormap
    
            For a list of available colormaps: 
                https://matplotlib.org/3.1.1/gallery/color/colormap_reference.html
    '''
    
    import matplotlib
    import numpy as np
    cmap = matplotlib.cm.get_cmap(map_name)
    n = np.linspace(0,N,N)/N
    return cmap(n)

def PlotGroups(points, groups, colors, ec='black', ax='None'):
    '''Function makes a scatter plot, given:
            Input:  points (array)
                    groups (an integer label for each point)
                    colors (one rgb tuple for each group)
                    ec (edgecolor for markers, default is black)
                    ax (optional handle to an existing axes object to add the new plot on top of)
            Output: handles to the figure (fig) and axes (ax) objects
    '''
    import matplotlib.pyplot as plt
    # Create a new figure if none was passed
    if ax == 'None':
        fig,ax = plt.subplots()
    else:
        fig = plt.gcf()
        
    for i in np.unique(groups):
        idx = (groups==i)
        ax.scatter(points[idx,0], points[idx,1],
                    color=colors[i],edgecolor=ec,
                    label = 'Group ' + str(i), alpha=0.5)
    ax.set_xlabel('$x_1$')
    ax.set_ylabel('$x_2$')
    ax.legend(bbox_to_anchor=[1,0.5],loc='center left')
    return fig,ax

def CompareClasses(actual, predicted, names=None):
    '''Function returns a confusion matrix, and overall accuracy given:
            Input:  actual - a list of actual classifications
                    predicted - a list of predicted classifications
                    names (optional) - a list of class names
    '''
    
    import pandas as pd
    accuracy = sum(actual==predicted)/actual.shape[0]
    classes = pd.DataFrame(columns=['Actual','Predicted'])
    classes['Actual'] = actual
    classes['Predicted'] = predicted
    conf_mat = pd.crosstab(classes['Predicted'],classes['Actual'])
    # Relabel the rows/columns if names was provided
    if type(names) != type(None):
        conf_mat.index=y_names
        conf_mat.index.name='Predicted'
        conf_mat.columns=y_names
        conf_mat.columns.name = 'Actual'
    print('Accuracy = ' + format(accuracy, '.2f'))
    return conf_mat, accuracy

# Function taken verbatim from: 
#   https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html
# Modified to include x and y labels
def plot_dendrogram(model, **kwargs):
    from scipy.cluster.hierarchy import dendrogram
    # Create linkage matrix and then plot the dendrogram
    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack([model.children_, model.distances_,
                                      counts]).astype(float)
    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)
    # Add axis labels
    plt.xlabel('Data Point')
    plt.ylabel('Distance')

"""# Agglomerative Clustering with TSNE"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering as AC

ac=AC(n_clusters=None, distance_threshold=0)
ac.fit(X)
plt.figure(figsize=(12,6))
plot_dendrogram(ac)
plt.xticks([])
plt.show()

from sklearn.manifold import TSNE
tsne=TSNE(random_state=146)
Xtsne=tsne.fit_transform(X)

n=2
ac=AC(n_clusters=n)
clusters=ac.fit_predict(X)

colors=GetColors(n)
PlotGroups(Xtsne,clusters, colors)
plt.xlabel('Tsne_1')
plt.ylabel('Tsne_2')
plt.show()

CompareClasses(y, clusters, y_names)

"""# Agglomerative Clustering Standardized TSNE"""

#NOW SCALED
ac=AC(n_clusters=None, distance_threshold=0)
ac.fit(X_scaled)

plt.figure(figsize=(12,6))
plot_dendrogram(ac)
plt.xticks([])
plt.show()

n=2
ac=AC(n_clusters=n)
clusters=ac.fit_predict(X_scaled)

Xs_tsne=tsne.fit_transform(X_scaled)

PlotGroups(Xs_tsne, clusters, colors)
plt.xlabel('Tsne_1')
plt.ylabel('Tsne_2')
plt.show()

CompareClasses(y,clusters,y_names)

"""# Agglomerative Clustering Standardized PCA"""

n=2
ac=AC(n_clusters=n)
clusters=ac.fit_predict(X_scaled)

pca=PCA()
Xp=pca.fit_transform(X_scaled)
PlotGroups(Xp, clusters, colors)
plt.xlabel('PC_1')
plt.ylabel('PC_2')

CompareClasses(y,clusters,y_names)

"""# DBSCAN Scaled TNSE and PCA"""

from sklearn.cluster import DBSCAN
e=2
m=4

db=DBSCAN(eps=e, min_samples=m)
clusters=db.fit_predict(X_scaled)

n=len(np.unique(clusters))
colors=GetColors(n)

from sklearn.decomposition import PCA
pca=PCA()
Xp=pca.fit_transform(X_scaled)


PlotGroups(Xp, clusters, colors)
plt.xlabel('PC_1')
plt.ylabel('PC_2')
plt.axis('equal')
plt.show()

from sklearn.manifold import TSNE
tsne=TSNE(random_state=146)

Xt=tsne.fit_transform(X_scaled)
PlotGroups(Xt, clusters, colors)
plt.xlabel('tsne_1')
plt.ylabel('tsne_2')
plt.axis('equal')
plt.show()

"""# KMeans Clustering with standardized TSNE and PCA"""

from sklearn.cluster import KMeans
k=2
km=KMeans(n_clusters=k, random_state=146)
km.fit(X_scaled)
y_pred=km.predict(X_scaled)
pca=PCA()
Xs_pca=pca.fit_transform(X_scaled)

colors=GetColors(k)
PlotGroups(Xs_pca, y_pred, colors)
plt.xlabel('PC_1')
plt.ylabel('PC_2')
plt.title('Kmeans with k=2 on standardized data')
plt.show()

from sklearn.manifold import TSNE
tsne=TSNE(random_state=146)
X_tsne=tsne.fit_transform(X_scaled)
PlotGroups(X_tsne, y_pred, colors)
plt.xlabel('tsne_1')
plt.ylabel('tsne_2')
plt.title('Kmeans with k=2 on standardized data')
plt.show()

CompareClasses(y,y_pred, y_names)

"""# Logistic Regression"""

def converter(Disorder):
    if Disorder == 'Migraine':
        return 0
    else:
        return 1
ytrain = df['Disorder'].apply(converter)
y_eval= ytrain

dftrain=X
filename='/content/gdrive/My Drive/FInaltest.csv'
dfeval=pd.read_csv(filename, header=None, sep=',')
header_row=0
dfeval.columns=dfeval.iloc[header_row]
dfeval=dfeval.drop(header_row)
dfeval=dfeval.dropna(axis=1, how='all')
dfeval=dfeval.drop('Disorder', axis=1)

dfeval['LDL Cholesterol mg/dL']=pd.to_numeric(dfeval['LDL Cholesterol mg/dL'])
dfeval['ceramides ng/ml']=pd.to_numeric(dfeval['ceramides ng/ml'])
dfeval['hippocampus mm^3']=pd.to_numeric(dfeval['hippocampus mm^3'])
dfeval['Glutamate ?mol/L']=pd.to_numeric(dfeval['Glutamate ?mol/L'])

dfeval['Systolic Blood Pressure']=pd.to_numeric(dfeval['Systolic Blood Pressure'])
dfeval['Ionized Magnesium Levels mmol/L']=pd.to_numeric(dfeval['Ionized Magnesium Levels mmol/L'])
dfeval['Amygdala Relative Volume (%)']=pd.to_numeric(dfeval['Amygdala Relative Volume (%)'])
dfeval['neocortical 5-HT4 receptor binding']=pd.to_numeric(dfeval['neocortical 5-HT4 receptor binding'])
dfeval['Cortisol Levels mcg/dL']=pd.to_numeric(dfeval['Cortisol Levels mcg/dL'])

import tensorflow as tf
tf.random.set_seed(146)

fc = tf.feature_column
NUMERIC_COLUMNS = X_names


def one_hot_cat_column(feature_name, vocab):
  return fc.indicator_column(
      fc.categorical_column_with_vocabulary_list(feature_name,
                                                 vocab))
feature_columns = []
for feature_name in NUMERIC_COLUMNS:
  feature_columns.append(fc.numeric_column(feature_name,
                                           dtype=tf.float32))

NUM_EXAMPLES = len(ytrain)

def make_input_fn(X, y, n_epochs=None, shuffle=True):
  def input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((X.to_dict(orient='list'), y))
    if shuffle:
      dataset = dataset.shuffle(NUM_EXAMPLES)
    
    dataset = (dataset
      .repeat(n_epochs)
      .batch(NUM_EXAMPLES))
    return dataset
  return input_fn

train_input_fn = make_input_fn(dftrain, ytrain)
eval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)



linear_est = tf.estimator.LinearClassifier(feature_columns)

from IPython.display import clear_output

# Train model.
linear_est.train(train_input_fn, max_steps=100)

# Evaluation.
result = linear_est.evaluate(eval_input_fn)
clear_output()
print(pd.Series(result))

"""# Gradient Boosted Trees"""

params = {
  'n_trees': 50,
  'max_depth': 3,
  'n_batches_per_layer': 1,
  #center_bias = True
  # to get DFCs. This will force the model to
  # make an initial prediction before using any features (e.g. use the mean of
  # the training labels for regression or log odds for classification when
  # using cross entropy loss).
  'center_bias': True
}

est = tf.estimator.BoostedTreesClassifier(feature_columns, **params)
# Train model.
est.train(train_input_fn, max_steps=100)

# Evaluation.
results = est.evaluate(eval_input_fn)
clear_output()
pd.Series(results).to_frame()

in_memory_params = dict(params)
in_memory_params['n_batches_per_layer'] = 1
# In-memory input_fn does not use batching.
def make_inmemory_train_input_fn(X, y):
  y = np.expand_dims(y, axis=1)
  def input_fn():
    return dict(X), y
  return input_fn
train_input_fn = make_inmemory_train_input_fn(dftrain, ytrain)

# Train the model.
est = tf.estimator.BoostedTreesClassifier(
    feature_columns, 
    train_in_memory=True, 
    **in_memory_params)

est.train(train_input_fn)
print(est.evaluate(eval_input_fn))

pred_dicts = list(est.experimental_predict_with_explanations(eval_input_fn))

labels = y_eval.values
probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])
df_dfc = pd.DataFrame([pred['dfc'] for pred in pred_dicts])

# Sum of DFCs + bias == probabality.
bias = pred_dicts[0]['bias']
dfc_prob = df_dfc.sum(axis=1) + bias
np.testing.assert_almost_equal(dfc_prob.values,
                               probs.values)

# Boilerplate code for plotting :)
def _get_color(value):
    """To make positive DFCs plot green, negative DFCs plot red."""
    green, red = sns.color_palette()[2:4]
    if value >= 0: return green
    return red

def _add_feature_values(feature_values, ax):
    """Display feature's values on left of plot."""
    x_coord = ax.get_xlim()[0]
    OFFSET = 0.15
    for y_coord, (feat_name, feat_val) in enumerate(feature_values.items()):
        t = plt.text(x_coord, y_coord - OFFSET, '{}'.format(feat_val), size=12)
        t.set_bbox(dict(facecolor='white', alpha=0.5))
    from matplotlib.font_manager import FontProperties
    font = FontProperties()
    font.set_weight('bold')
    t = plt.text(x_coord, y_coord + 1 - OFFSET, 'feature\nvalue',
    fontproperties=font, size=12)

def plot_example(example):
  TOP_N = 8 # View top 8 features.
  sorted_ix = example.abs().sort_values()[-TOP_N:].index  # Sort by magnitude.
  example = example[sorted_ix]
  colors = example.map(_get_color).tolist()
  ax = example.to_frame().plot(kind='barh',
                          color=[colors],
                          legend=None,
                          alpha=0.75,
                          figsize=(10,6))
  ax.grid(False, axis='y')
  ax.set_yticklabels(ax.get_yticklabels(), size=14)

  # Add feature values.
  _add_feature_values(dfeval.iloc[ID][sorted_ix], ax)
  return ax

ID = 40
example = df_dfc.iloc[ID]  # Choose ith example from evaluation set.
TOP_N = 8  # View top 8 features.
sorted_ix = example.abs().sort_values()[-TOP_N:].index
ax = plot_example(example)
ax.set_title('Feature contributions for example {}\n pred: {:1.2f}; label: {}'.format(ID, probs[ID], labels[ID]))
ax.set_xlabel('Contribution to predicted probability', size=14)
plt.show()

# Boilerplate plotting code.
def dist_violin_plot(df_dfc, ID):
  # Initialize plot.
  fig, ax = plt.subplots(1, 1, figsize=(10, 6))

  # Create example dataframe.
  TOP_N = 8  # View top 8 features.
  example = df_dfc.iloc[ID]
  ix = example.abs().sort_values()[-TOP_N:].index
  example = example[ix]
  example_df = example.to_frame(name='dfc')

  # Add contributions of entire distribution.
  parts=ax.violinplot([df_dfc[w] for w in ix],
                 vert=False,
                 showextrema=False,
                 widths=0.7,
                 positions=np.arange(len(ix)))
  face_color = sns_colors[0]
  alpha = 0.15
  for pc in parts['bodies']:
      pc.set_facecolor(face_color)
      pc.set_alpha(alpha)

  # Add feature values.
  _add_feature_values(dfeval.iloc[ID][sorted_ix], ax)

  # Add local contributions.
  ax.scatter(example,
              np.arange(example.shape[0]),
              color=sns.color_palette()[2],
              s=100,
              marker="s",
              label='contributions for example')

  # Legend
  # Proxy plot, to show violinplot dist on legend.
  ax.plot([0,0], [1,1], label='eval set contributions\ndistributions',
          color=face_color, alpha=alpha, linewidth=10)
  legend = ax.legend(loc='lower right', shadow=True, fontsize='x-large',
                     frameon=True)
  legend.get_frame().set_facecolor('white')

  # Format plot.
  ax.set_yticks(np.arange(example.shape[0]))
  ax.set_yticklabels(example.index)
  ax.grid(False, axis='y')
  ax.set_xlabel('Contribution to predicted probability', size=14)

dist_violin_plot(df_dfc, ID)
plt.title('Feature contributions for example {}\n pred: {:1.2f}; label: {}'.format(ID, probs[ID], labels[ID]))
plt.show()

